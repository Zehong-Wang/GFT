pretrain_dataset: all
weight_setting: all
input_dim: 768
hidden_dim: 768
num_layers: 2
activation: relu
backbone: sage
normalize: batch
dropout: 0.15
code_dim: 768
codebook_size: 128
codebook_head: 4
codebook_decay: 0.8
commit_weight: 10
ortho_reg_weight: 1
ortho_reg_max_codes: 32
pretrain_epochs: 50
pretrain_lr: 0.0001
pretrain_weight_decay: 0.00001
pretrain_batch_size: 1024
feat_p: 0.2
edge_p: 0.2
topo_recon_ratio: 0.1
feat_lambda: 100
topo_lambda: 0.01
topo_sem_lambda: 100
sem_lambda: 1
sem_encoder_decay: 0.99
use_schedular: True